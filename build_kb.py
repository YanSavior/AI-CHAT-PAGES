#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰‹åŠ¨æ„å»ºçŸ¥è¯†åº“ - å¤„ç†çœŸå®æ¯•ä¸šç”Ÿæ•°æ®å’ŒåŸ¹å…»æ–¹æ¡ˆ
"""

import pandas as pd
import os
import chromadb
from sentence_transformers import SentenceTransformer
import numpy as np
import re

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    if pd.isna(text):
        return ""
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', str(text).strip())
    return text

def split_cultivation_plan(content):
    """æ™ºèƒ½åˆ†å‰²åŸ¹å…»æ–¹æ¡ˆå†…å®¹"""
    sections = []
    
    # å®šä¹‰ä¸»è¦ç« èŠ‚æ¨¡å¼
    patterns = [
        r'ä¸€ã€ä¸“ä¸šä»‹ç»',
        r'äºŒã€åŸ¹å…»ç›®æ ‡ä¸æ¯•ä¸šè¦æ±‚',
        r'ä¸‰ã€ä¸»å¹²å­¦ç§‘ä¸ä¸“ä¸šæ ¸å¿ƒè¯¾',
        r'å››ã€è¯¾ç¨‹è®¾ç½®åŠå­¦åˆ†åˆ†é…è¡¨',
        r'äº”ã€è¯¾ç¨‹ä½“ç³»é…ç½®æµç¨‹å›¾',
        r'å…­ã€æŒ‡å¯¼æ€§æ•™å­¦è®¡åˆ’è¿›ç¨‹',
        r'ä¸ƒã€å®è·µæ•™å­¦',
        r'å…«ã€æ¯•ä¸šè¦æ±‚',
        r'ä¹ã€å°±ä¸šæ–¹å‘',
        r'åã€ç»§ç»­æ·±é€ '
    ]
    
    # æŸ¥æ‰¾æ‰€æœ‰ç« èŠ‚ä½ç½®
    positions = []
    for pattern in patterns:
        matches = list(re.finditer(pattern, content))
        positions.extend(matches)
    
    # æŒ‰ä½ç½®æ’åº
    positions.sort(key=lambda x: x.start())
    
    # åˆ†å‰²å†…å®¹
    for i, pos in enumerate(positions):
        start = pos.start()
        if i + 1 < len(positions):
            end = positions[i + 1].start()
        else:
            end = len(content)
        
        section_content = content[start:end].strip()
        if section_content:
            sections.append(section_content)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç« èŠ‚ï¼ŒæŒ‰æ®µè½åˆ†å‰²
    if not sections:
        paragraphs = content.split('\n\n')
        sections = [p.strip() for p in paragraphs if p.strip()]
    
    return sections

def build_knowledge_base():
    """æ„å»ºçŸ¥è¯†åº“"""
    
    print("ğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†åº“...")
    
    # åˆå§‹åŒ–å‘é‡åŒ–å™¨
    print("ğŸ”„ åŠ è½½BGEæ¨¡å‹...")
    try:
        model = SentenceTransformer("D:/bge_models/bge-small-zh-v1.5")
    except:
        print("âš ï¸ æœ¬åœ°æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨åœ¨çº¿æ¨¡å‹...")
        model = SentenceTransformer("BAAI/bge-small-zh-v1.5")
    print("âœ… BGEæ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åˆå§‹åŒ–ChromaDB
    print("ğŸ”„ åˆå§‹åŒ–ChromaDB...")
    client = chromadb.PersistentClient(path="./vector_db")
    
    # åˆ é™¤å·²å­˜åœ¨çš„é›†åˆ
    try:
        client.delete_collection("student_knowledge")
        print("ğŸ—‘ï¸ åˆ é™¤æ—§é›†åˆ")
    except:
        pass
    
    collection = client.create_collection(
        name="student_knowledge",
        metadata={"hnsw:space": "cosine"}
    )
    print("âœ… ChromaDBåˆå§‹åŒ–å®Œæˆ")
    
    all_documents = []
    all_metadatas = []
    doc_id_counter = 0
    
    # å¤„ç†çœŸå®æ¯•ä¸šç”Ÿæ•°æ®
    print("ğŸ“Š å¤„ç†çœŸå®æ¯•ä¸šç”Ÿæ•°æ®...")
    try:
        graduates_df = pd.read_csv("data/real_graduates.csv", encoding='utf-8')
        print(f"è¯»å–åˆ° {len(graduates_df)} æ¡æ¯•ä¸šç”Ÿè®°å½•")
        
        for idx, row in graduates_df.iterrows():
            # ç”Ÿæˆæ¯•ä¸šç”Ÿæè¿°
            description = f"æ¯•ä¸šç”Ÿ{clean_text(row['å§“å'])}ï¼Œå­¦å·{clean_text(row['å­¦å·'])}ï¼Œ"
            description += f"GPAæˆç»©{clean_text(row['GPA'])}ï¼Œæ‰€åœ¨åœ°{clean_text(row['æ‰€åœ¨åœ°'])}ï¼Œ"
            description += f"å‘å±•æ–¹å‘{clean_text(row['å‘å±•æ–¹å‘'])}ï¼Œ"
            
            if pd.notna(row['å°±ä¸šå»å‘']) and clean_text(row['å°±ä¸šå»å‘']) != '':
                description += f"å°±ä¸šå»å‘{clean_text(row['å°±ä¸šå»å‘'])}ï¼Œ"
            
            if pd.notna(row['å¹´è–ª']) and clean_text(row['å¹´è–ª']) != '':
                description += f"å¹´è–ª{clean_text(row['å¹´è–ª'])}"
            else:
                description += "å¹´è–ªä¿¡æ¯æœªæä¾›"
            
            all_documents.append(description)
            all_metadatas.append({
                "type": "graduate",
                "student_id": clean_text(row['å­¦å·']),
                "name": clean_text(row['å§“å']),
                "gpa": clean_text(row['GPA']),
                "location": clean_text(row['æ‰€åœ¨åœ°']),
                "career_path": clean_text(row['å‘å±•æ–¹å‘']),
                "employment": clean_text(row['å°±ä¸šå»å‘']),
                "salary": clean_text(row['å¹´è–ª'])
            })
            doc_id_counter += 1
            
    except Exception as e:
        print(f"âŒ å¤„ç†æ¯•ä¸šç”Ÿæ•°æ®æ—¶å‡ºé”™: {e}")
    
    # å¤„ç†åŸ¹å…»æ–¹æ¡ˆæ•°æ®
    print("ğŸ“‹ å¤„ç†åŸ¹å…»æ–¹æ¡ˆæ•°æ®...")
    try:
        with open("data/all_cultivation_plans.txt", "r", encoding="utf-8") as f:
            plan_content = f.read()
        
        # æŒ‰ä¸“ä¸šåˆ†å‰²
        major_sections = re.split(r'## (.+?) ä¸“ä¸šä¸“ä¸šåŸ¹å…»æ–¹æ¡ˆ', plan_content)
        
        for i in range(1, len(major_sections), 2):
            if i + 1 < len(major_sections):
                major_name = major_sections[i].strip()
                major_content = major_sections[i + 1].strip()
                
                if major_name and major_content:
                    # åˆ†å‰²ä¸“ä¸šå†…å®¹
                    sections = split_cultivation_plan(major_content)
                    
                    for j, section in enumerate(sections):
                        if len(section.strip()) > 50:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                            all_documents.append(section.strip())
                            all_metadatas.append({
                                "type": "cultivation_plan",
                                "major": major_name,
                                "section_id": j,
                                "section_name": section[:50] + "..." if len(section) > 50 else section
                            })
                            doc_id_counter += 1
                            
    except Exception as e:
        print(f"âŒ å¤„ç†åŸ¹å…»æ–¹æ¡ˆæ•°æ®æ—¶å‡ºé”™: {e}")
    
    print(f"ğŸ“ æ€»å…± {len(all_documents)} ä¸ªæ–‡æ¡£")
    
    if len(all_documents) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£ï¼ŒçŸ¥è¯†åº“æ„å»ºå¤±è´¥")
        return None
    
    # å‘é‡åŒ–æ–‡æ¡£
    print("ğŸ”„ å‘é‡åŒ–æ–‡æ¡£...")
    try:
        embeddings = model.encode(all_documents, show_progress_bar=True, batch_size=32)
        
        # ç”Ÿæˆæ–‡æ¡£ID
        doc_ids = [f"doc_{i}" for i in range(len(all_documents))]
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        print("ğŸ’¾ å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
        collection.add(
            documents=all_documents,
            embeddings=embeddings.tolist(),
            metadatas=all_metadatas,
            ids=doc_ids
        )
        
        print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        graduate_count = sum(1 for meta in all_metadatas if meta['type'] == 'graduate')
        plan_count = sum(1 for meta in all_metadatas if meta['type'] == 'cultivation_plan')
        
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - æ¯•ä¸šç”Ÿæ–‡æ¡£: {graduate_count} ä¸ª")
        print(f"  - åŸ¹å…»æ–¹æ¡ˆæ–‡æ¡£: {plan_count} ä¸ª")
        print(f"  - æ€»æ–‡æ¡£æ•°: {len(all_documents)} ä¸ª")
        print(f"  - å‘é‡ç»´åº¦: {embeddings.shape[1]}")
        
        return collection
        
    except Exception as e:
        print(f"âŒ å‘é‡åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return None

def test_knowledge_base(collection):
    """æµ‹è¯•çŸ¥è¯†åº“"""
    if collection is None:
        print("âŒ çŸ¥è¯†åº“æœªæ„å»ºæˆåŠŸï¼Œæ— æ³•æµ‹è¯•")
        return
    
    print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢:")
    test_questions = [
        "æœ‰å“ªäº›é«˜è–ªå°±ä¸šçš„æ¯•ä¸šç”Ÿï¼Ÿ",
        "å¾®ç”µå­ç§‘å­¦ä¸å·¥ç¨‹ä¸“ä¸šçš„åŸ¹å…»ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ",
        "GPAæˆç»©é«˜çš„å­¦ç”Ÿå°±ä¸šæƒ…å†µå¦‚ä½•ï¼Ÿ",
        "æœ‰å“ªäº›æ ¸å¿ƒè¯¾ç¨‹ï¼Ÿ",
        "äººå·¥æ™ºèƒ½æ–¹å‘çš„æ¯•ä¸šç”Ÿå»å‘å¦‚ä½•ï¼Ÿ"
    ]
    
    try:
        model = SentenceTransformer("D:/bge_models/bge-small-zh-v1.5")
    except:
        model = SentenceTransformer("BAAI/bge-small-zh-v1.5")
    
    for question in test_questions:
        print(f"\né—®é¢˜: {question}")
        
        try:
            # å‘é‡åŒ–é—®é¢˜
            query_embedding = model.encode([question])
            
            # æœç´¢
            results = collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=3
            )
            
            print("ç›¸å…³æ–‡æ¡£:")
            for i, doc in enumerate(results['documents'][0]):
                metadata = results['metadatas'][0][i]
                print(f"  {i+1}. [{metadata['type']}] {doc[:150]}...")
                
        except Exception as e:
            print(f"  âŒ æŸ¥è¯¢å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("çŸ¥è¯†åº“æ„å»ºå·¥å…· - çœŸå®æ¯•ä¸šç”Ÿæ•°æ® + åŸ¹å…»æ–¹æ¡ˆ")
    print("=" * 60)
    
    # æ„å»ºçŸ¥è¯†åº“
    collection = build_knowledge_base()
    
    # æµ‹è¯•çŸ¥è¯†åº“
    test_knowledge_base(collection)
    
    print(f"\nğŸ‰ çŸ¥è¯†åº“æ„å»ºå’Œæµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ å‘é‡æ•°æ®åº“ä¿å­˜åœ¨: ./vector_db/")

if __name__ == "__main__":
    main() 